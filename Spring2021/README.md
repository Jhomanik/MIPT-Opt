# Spring term 2021

0. Some applications of convex optimizations ([ru](../preliminaries/demos/demos.ipynb))

1. Intro to numerical optimization methods. Gradient descent ([ru](./intro_gd.ipynb))

2. How to accelerate gradient descent: conjugate gradient method, heavy-ball method and fast gradient method

3. Second order methods: Newton method. Quasi-Newton methods as trade-off between convergence speed and cost of one iterations

4. Non-smooth optimization problems: subgradient methods and intro to proximal methods

5. Least squares problem: matrix factorizations and Levenberg-Marquardt algorithm

6. Smoothing: smooth minimization of non-smooth functions ([original paper](https://link.springer.com/article/10.1007/s10107-004-0552-5)) 

7. Simple constrained optimization problems: projected gradient method and Frank-Wolfe method

8. General purpose solvers: interior point methods

9. How to parallelize optimization methods: penalty method, augmented Lagrangian method and ADMM

10. Coordinate-wise methods
