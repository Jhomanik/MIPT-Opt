# Spring term 2021

0. Some applications of convex optimizations ([ru](../preliminaries/demos/demos.ipynb))

1. Intro to numerical optimization methods. Gradient descent ([ru](./intro_gd.ipynb))

2. How to accelerate gradient descent: conjugate gradient method, heavy-ball method and fast gradient method ([vol 1, ru](./acc_grad.ipynb); [vol 2, ru](./acc_grad_vol2.ipynb))

3. Second order methods: Newton method. Quasi-Newton methods as trade-off between convergence speed and cost of one iterations ([ru](./newton_quasi.ipynb))

4. Non-smooth optimization problems: subgradient methods and intro to proximal methods ([en](./subgrad_prox.ipynb))

5. Smoothing: smooth minimization of non-smooth functions ([original paper](https://link.springer.com/article/10.1007/s10107-004-0552-5)) ([ru](./smoothing.ipynb))

6. Simple constrained optimization problems: projected gradient method and Frank-Wolfe method

7. General purpose solvers: interior point methods

8. How to parallelize optimization methods: penalty method, augmented Lagrangian method and ADMM

9. Coordinate-wise methods
